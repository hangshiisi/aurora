\documentclass[fontsize=11pt,paper=a4,pagesize=auto]{report}

\usepackage[english]{babel}
\usepackage{microtype}
\usepackage{blindtext}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{csquotes}


\lstset{
  language=C,
  basicstyle=\small,
  breaklines=true}
\frenchspacing 
\begin{document}

\begin{titlepage}
  \vspace*{1cm}
  {\huge\raggedright Report of Captone Project Task 1\par}
  \noindent\hrulefill\par
  {\LARGE\raggedleft A Student in Coursera\par}
  \vfill
  {\Large\raggedleft 10/31/2017\par}
\end{titlepage}

%\tableofcontents{}


\section{Overview}
This report covers the design and implementation details for Cloud Computing Captone Project Task 1. The goals of this task are to perform the following:
\begin{itemize}
\item Extract and clean the transportation dataset, and then store the result in HDFS.
\item Answer 2 questions from Group 1, 3 questions from Group 2, and both questions from Group 3 using Hadoop. Store the results for questions from Group 2 and Question 3.2 in Cassandra.   
\end{itemize}
 
\section{Data Cleanup}
\par The first task is to clean and store the dataset. We  first retrieve and extract the dataset from the EBS volume snapshot by following steps in \url{http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html}. One key thing to note is that we need to create EC2 instance in the same region and availabbility zone as the data EBS snapshot (snap-e1608d88 in us-east-1d).   
\par Afterwards, we explore the data set and decide on how we wish to clean it. Then we ignore unrelated rows and extract only the columns from the csv files. The raw data is in \path{/data/aviation/airline_ontime} directory. 
\par Approaches: Two ways have been used to do the cleanup: 
\begin{itemize} 
\item A python script is developed on top of \href{https://docs.python.org/2/library/csv.html}{python CSV package} to achieve this. 
\item Pig script, see \url{https://pig.apache.org/} for details. 
\end{itemize}

\par Such data cleanup is a one-time job and below are the fields we extracted: 
\begin{itemize}
\item       AirlineID:chararray, --a8
\item       Carrier:chararray,   --a9
\item       FlightNum:int,       --a11
\item       Origin:chararray,    --a12       
\item       Dest:chararray,      --a18
\item       DepDelay:float,      --a26
\item       DepDel15:float,      --a28
\item       ArrDelay:float,      --a37 
\item       ArrDel15:float       --a39
\end{itemize}

\par The cleaned data are stored on HDFS via below hdfs dfs command  as tab delimetered fields with each line indicating one flight. 
\begin{lstlisting}{centered}
	hdfs dfs -copyFromLocal <local files> <hdfs dir>  
\end{lstlisting}
 
\section{System Overview}
\subsection{Hadoop MapReduce}
The system is essentially a 4 node Apache Hadoop cluster with one namenode and three datanodes running on AWS EC2. 
Hadoop Yarn runs and serves as the resource manager and application manager.

\subsection{Cassandra} 
Cassandra cluster is deployed on the three Hadoop datanodes to provide data replication and scalability. 
The cluster run on the same rack and utilizes GossipingPropertyFileSnitch scheme. One of the nodes is assigned as the seed provider.
 
\section{Group 1 Problems} 
\begin{lstlisting}[language = C++]
// include standard input/output stream objects:
#include <iostream>
// the main method:
int main()
{
    std::cout << "Hello TeX world!" << std::endl;
}
\end{lstlisting}
 
\section{Group 2 Problems}
 

\blindtext

\section{Group 3 Problems}
 

\blindtext

\section{Optimization Techniques}
 

\blindtext
 
\section{Result Measurement}
Correctness 

Speed 

Video clearly shows both of the following: 1) Ingesting and analyzing data in the systems; 2) Querying results.


 
\end{document}

